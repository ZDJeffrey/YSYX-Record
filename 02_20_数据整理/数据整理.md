# 数据整理

> 统计words文件(`/usr/share/dict/words`)中包含至少三个`a`且不以`'s`结尾的单词个数。这些单词中，出现频率前三的末尾两个字母是什么？`sed`的`y`命令，或者`tr`程序也许可以帮你解决大小写的问题。共存在多少种词尾两字母组合？还有一个很有挑战性的问题：哪个组合从未出现过？

- 找出符合要求的单词的词尾
    ```bash
    cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | sed -E "s/.*(\w{2})$/\1/"
    ```
- 使用`sort`后`uniq -c`可进行统计，而通过`sort -n`进行数量排序，`tail -n3`查看出现频率前三的后缀
- 获取所有两个字符组成的单词，写入`all.txt`中
    ```bash
    #!/bin/bash
    for i in {a..z};do
        for j in {a..z};do
            echo "$i$j"
        done
    done
    ```
- 将符合要求的后缀写入`occurance.txt`中

    ```bash
    cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | sed -E "s/.*(\w{2})$/\1/" | sort | uniq > occurance.txt
    ```
- 使用`diff`对比两个文件，并设定输出格式
    ```bash
    diff --unchanged-group-format='' occurance.txt all.txt
    ```

> 进行原地替换听上去很有诱惑力，例如：`sed s/REGEX/SUBSTITUTION/ input.txt > input.txt`。但是这并不是一个明智的做法，为什么呢？还是说只有`sed`是这样的? 查看`man sed`来完成这个问题

因为在`sed`指令执行前，会先清空文件`input.txt`，从而导致文件内容在处理就丢失了。
可以使用`-i`选项进行原地替换。
此外，若需要对原文件进行备份处理，可以通过`-i.bak`生成备份文件。

> 找出您最近十次开机的开机时间平均数、中位数和最长时间。在Linux上需要用到`journalctl`，而在`macOS`上使用`log show`。找到每次起到开始和结束时的时间戳。在Linux上类似这样操作：
> ```
> Logs begin at ...
> ```
> 和
> ```
> systemd[577]: Startup finished in ...
> ```

- 获取启动时间
    ```bash
    #!/bin/bash
    for i in {0..9};do
        journalctl -b-$i | grep "Startup finished in" | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"
    done
    ```
- 最长时间
    ```bash
    ./getlog.sh | sort | tail -n1
    ```
- 最短时间
    ```bash
    ./getlog.sh | sort | head -n1
    ```
- 平均数
    ```bash
    ./getlog.sh | paste -sd+ | bc -l | awk '{print $1/10}'
    ```
- 中位数
    ```bash
    ./getlog.sh | sort |paste -sd\  | awk '{print ($5+$6)/2}'
    ```
> 查看之前三次重启启动信息中不同的部分(参见`journalctl`的`-b`选项)。将这一任务分为几个步骤，首先获取之前三次启动的启动日志，也许获取启动日志的命令就有合适的选项可以帮助您提取前三次启动的日志，亦或者您可以使用`sed '0,/STRING/d'`来删除STRING匹配到的字符串前面的全部内容。然后，过滤掉每次都不相同的部分，例如时间戳。下一步，重复记录输入行并对其计数(可以使用`uniq`)。最后，删除所有出现过3次的内容（因为这些内容是三次启动日志中的重复部分）。

- 获取前三次启动的启动日志
    ```bash
    #!/bin/bash
    for i in {0..2};do
        journalctl -b-$i
    done
    ```
- 使用`sed`删除时间戳，`sort | uniq -c`进行统计，最后使用`awk`删选重复次数为3的内容
    ```bash
    ./getlog.sh | sed -E "s/.*Legion\ (.*)/\1/" | sort | uniq -c | sort | awk '$1<3 { print }'
    ```

> 在网上找一个类似[这个](https://stats.wikimedia.org/EN/TablesWikipediaZZ.htm)或者[这个](https://ucr.fbi.gov/crime-in-the-u.s/2016/crime-in-the-u.s.-2016/topic-pages/tables/table-1)的数据集。或者从[这里](https://www.springboard.com/blog/free-public-data-sets-data-science-project/)找一些。使用`curl`获取数据集并提取其中两列数据，如果您想要获取的是HTML数据，那么`pup`可能会更有帮助。对于JSON类型的数据，可以试试`jq`。请使用一条指令来找出其中一列的最大值和最小值，用另外一条指令计算两列之间差的总和。

- 提取数据
    ```bash
    curl 'https://stats.wikimedia.org/EN/TablesWikipediaZZ.htm#wikipedians' | sed -n "/table1/,/<\/table>/p" | grep "<tr" | sed "1,12d" | head -n -3 | sed -E "s/(<[^>]*>)+/ /g" | sed "s/ &nbsp;/ -/g" | sed "s/&nbsp;//g" > data
    ```

- 找出最大值
    ```bash
    awk '{print $1,$4,$5}' data | sort --key=2 -n | head -n1
    ```
- 找出最小值
    ```bash
    awk '{print $1,$4,$5}' data | sort --key=2 -n | tail -n1
    ```
- 计算两列之差总和
    ```bash
    awk '{print $4-$5}' data | awk '{s+=$1} END {print s}'
    ```